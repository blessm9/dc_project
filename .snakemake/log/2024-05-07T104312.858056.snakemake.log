Building DAG of jobs...
Provided cores: 20
Rules claiming more threads will be scaled down.
Job stats:
job             count
------------  -------
setup_docker        1
total               1

Select jobs to execute...
Execute 1 jobs...

[Tue May  7 10:43:12 2024]
localrule setup_docker:
    input: 00 - Utils/Docker/Dockerfile, 00 - Utils/Docker/requirements.txt
    log: logs/setup_docker.log
    jobid: 0
    reason: Forced execution
    resources: tmpdir=C:\Users\Etienne\AppData\Local\Temp

Traceback (most recent call last):

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\cli.py", line 2075, in args_to_api
    dag_api.execute_workflow(

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\api.py", line 589, in execute_workflow
    workflow.execute(

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\workflow.py", line 1247, in execute
    raise e

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\workflow.py", line 1243, in execute
    success = self.scheduler.schedule()
              ^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\scheduler.py", line 313, in schedule
    self.run(

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\scheduler.py", line 406, in run
    executor.run_jobs(jobs)

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake_interface_executor_plugins\executors\base.py", line 72, in run_jobs
    self.run_job(job)

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\executors\local.py", line 106, in run_job
    future = self.run_single_job(job)
             ^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\executors\local.py", line 182, in run_single_job
    self.cached_or_run, job, run_wrapper, *self.job_args_and_prepare(job)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\executors\local.py", line 114, in job_args_and_prepare
    async_run(job.prepare())

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\common\__init__.py", line 94, in async_run
    return asyncio.run(coroutine)
           ^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\asyncio\base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\jobs.py", line 760, in prepare
    await self.remove_existing_output()

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\jobs.py", line 736, in remove_existing_output
    await f.remove(remove_non_empty_dir=False)

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\io.py", line 684, in remove
    await remove(

  File "C:\Users\Etienne\AppData\Local\Programs\Python\Python311\Lib\site-packages\snakemake\io.py", line 983, in remove
    os.remove(file)

PermissionError: [WinError 32] Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird: 'logs/setup_docker.log'

