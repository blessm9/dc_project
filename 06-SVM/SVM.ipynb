{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02720919-16ea-4152-bc91-ecce4ece6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from propy import PyPro\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "\n",
    "con = sqlite3.connect(\"../unified_CD2.db\")\n",
    "cur = con.cursor()\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM prod_desc\", con)\n",
    "df[\"AB\"] = df[\"AB\"].apply(lambda x:int.from_bytes(x,\"little\"))\n",
    "df = df[[\"seq\", \"AB\"]]\n",
    "\n",
    "csv_filename = \"etidata.csv\"\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af9d13-23d1-4601-9dfd-26353d4ce7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094fcc56-3a3c-40f6-9ae5-bccbfd3443b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  13%|███████▍                                                 | 1681/12801 [06:28<36:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KKKR: float division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  14%|███████▉                                                 | 1769/12801 [06:47<32:57,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KKRK: float division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  49%|███████████████████████████▋                             | 6214/12801 [22:45<28:04,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence RKKK: float division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:  52%|█████████████████████████████▍                           | 6625/12801 [24:11<28:07,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing sequence KRRRRRR: float division by zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|████████████████████████████████████████████████████████| 12801/12801 [46:34<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken: 2992.001680135727 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from propy import PyPro\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to extract all features and handle errors gracefully\n",
    "def extract_all_features(peptide):\n",
    "    try:\n",
    "        pro = PyPro.GetProDes(peptide)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Amino acid composition\n",
    "        features += list(pro.GetAAComp().values())\n",
    "        \n",
    "        # Dipeptide composition\n",
    "        features += list(pro.GetDPComp().values())\n",
    "        \n",
    "        # Tripeptide composition\n",
    "        features += list(pro.GetTPComp().values())\n",
    "        \n",
    "        # Moreau-Broto autocorrelation descriptors\n",
    "        features += list(pro.GetMoreauBrotoAuto().values())\n",
    "        \n",
    "        # Moran autocorrelation descriptors\n",
    "        features += list(pro.GetMoranAuto().values())\n",
    "        \n",
    "        # Geary autocorrelation descriptors\n",
    "        features += list(pro.GetGearyAuto().values())\n",
    "        \n",
    "        # Quasi-sequence order descriptors\n",
    "        features += list(pro.GetQSO().values())\n",
    "        \n",
    "        # Calculate additional physicochemical properties using Bio.SeqUtils.ProtParam\n",
    "        analysed_seq = ProteinAnalysis(peptide)\n",
    "        physchem_features = [\n",
    "            analysed_seq.molecular_weight(),  # Molecular weight\n",
    "            analysed_seq.isoelectric_point(),  # Isoelectric point (pI)\n",
    "            analysed_seq.instability_index(),  # Instability index\n",
    "            analysed_seq.gravy(),  # Hydrophobicity (GRAVY)\n",
    "        ]\n",
    "        features += physchem_features\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sequence {peptide}: {e}\")\n",
    "        # Return a list of NaNs of the same length as the feature vector\n",
    "        num_features = 544  # Total number of features\n",
    "        return [np.nan] * num_features\n",
    "\n",
    "# Apply function to each peptide sequence with multiprocessing\n",
    "num_processes = cpu_count()  # Number of CPU cores\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    features_list = list(tqdm(pool.imap(extract_all_features, df['seq']), total=len(df), desc=\"Extracting features\"))\n",
    "\n",
    "# Expand features into separate columns\n",
    "df_features = pd.DataFrame(features_list)\n",
    "\n",
    "# Define the feature names\n",
    "# Assuming you have already predefined feature names or generated them elsewhere\n",
    "\n",
    "# Concatenate original dataframe with features\n",
    "df_final = pd.concat([df, df_features], axis=1)\n",
    "\n",
    "csv_filename = \"peptidefeatures.csv\"\n",
    "df_final.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Calculate the total time taken\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b200f40-288b-4842-8a52-9d77a19a29bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to convert non-numerical values to NaN and handle errors gracefully\n",
    "def ensure_numerical(df):\n",
    "    # Iterate through each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        # Convert non-numerical values to NaN\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# Ensure all columns have numerical values\n",
    "df_final = ensure_numerical(df_final)\n",
    "\n",
    "# Relabel the columns appropriately if not already done\n",
    "# Assuming the column names have been assigned correctly as per the previous code\n",
    "# Here we can double-check and rename if necessary, or use the existing names\n",
    "\n",
    "example_peptide = df['seq'].iloc[0]\n",
    "\n",
    "aa_comp_names = [f\"AAComp_{aa}\" for aa in pro_example.GetAAComp().keys()]\n",
    "dp_comp_names = [f\"DPComp_{dp}\" for dp in pro_example.GetDPComp().keys()]\n",
    "tp_comp_names = [f\"TPComp_{tp}\" for tp in pro_example.GetTPComp().keys()]\n",
    "moreau_broto_names = [f\"MoreauBroto_{i}\" for i in range(len(pro_example.GetMoreauBrotoAuto().values()))]\n",
    "moran_names = [f\"Moran_{i}\" for i in range(len(pro_example.GetMoranAuto().values()))]\n",
    "geary_names = [f\"Geary_{i}\" for i in range(len(pro_example.GetGearyAuto().values()))]\n",
    "qso_names = [f\"QSO_{i}\" for i in range(len(pro_example.GetQSO().values()))]\n",
    "physchem_names = ['Molecular_Weight', 'Isoelectric_Point', 'Instability_Index', 'GRAVY']\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = (aa_comp_names + dp_comp_names + tp_comp_names + moreau_broto_names +\n",
    "                 moran_names + geary_names + qso_names + physchem_names)\n",
    "\n",
    "# Assign column names to the features dataframe\n",
    "df_features.columns = feature_names\n",
    "\n",
    "# Concatenate original dataframe with features\n",
    "df_final = pd.concat([df, df_features], axis=1)\n",
    "\n",
    "# Show the dataframe\n",
    "print(df_final)\n",
    "\n",
    "# Calculate the total time taken\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time taken: {total_time} seconds\")\n",
    "\n",
    "# Save the dataframe to a file to avoid running the feature extraction again\n",
    "df_final.to_csv('df_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1141a-db5d-44b5-a3d0-fab3a3f083aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Initial Model Training and Evaluation\n",
    "def initial_model_training_evaluation(X_train, X_test, y_train, y_test):\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    initial_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Initial Model Accuracy:\", initial_accuracy)\n",
    "    return clf\n",
    "\n",
    "# Step 3: Model Training and Evaluation after PCA\n",
    "def model_training_evaluation_after_pca(X_train, X_test, y_train, y_test, n_components):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    pca_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Model Accuracy after PCA:\", pca_accuracy)\n",
    "    return pca, clf\n",
    "\n",
    "# Step 5: Model Training and Evaluation after Feature Selection\n",
    "def model_training_evaluation_after_feature_selection(X_train, X_test, y_train, y_test, k_features):\n",
    "    selector = SelectKBest(f_classif, k=k_features)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "    fs_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Model Accuracy after Feature Selection:\", fs_accuracy)\n",
    "    return selector, clf\n",
    "\n",
    "# Step 7: Hyperparameter Tuning\n",
    "def hyperparameter_tuning(X_train, y_train):\n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', random_state=42), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    return best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71ae19f-ba1e-434a-b544-ea0c64b9d5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 seq  AAComp_A  AAComp_R  \\\n",
      "0                               ACDEFGHIKLMNPQRSTVWY     5.000     5.000   \n",
      "1  MRTLLVFLLLAIFVAVLIGNVQVEAACKEYWECGAFLFCIEGICVPMIG    10.204     2.041   \n",
      "\n",
      "   AAComp_N  AAComp_D  AAComp_C  AAComp_E  AAComp_Q  AAComp_G  AAComp_H  ...  \\\n",
      "0     5.000       5.0     5.000     5.000     5.000     5.000       5.0  ...   \n",
      "1     2.041       0.0     8.163     8.163     2.041     8.163       0.0  ...   \n",
      "\n",
      "     QSO_94    QSO_95    QSO_96    QSO_97    QSO_98    QSO_99  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.025391  0.023184  0.020166  0.023526  0.019069  0.013956   \n",
      "\n",
      "   Molecular_Weight  Isoelectric_Point  Instability_Index     GRAVY  \n",
      "0         2395.7134           6.784552          84.740000 -0.490000  \n",
      "1         5408.5940           4.493088          27.159184  1.485714  \n",
      "\n",
      "[2 rows x 9245 columns]\n",
      "Total time taken: 1.2066779136657715 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset into a DataFrame (assuming 'df' contains your data)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assuming 'df' contains your feature data and 'AB' is your target column\n",
    "X = df.drop(columns=['AB'])\n",
    "y = df['AB']\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Step 1: Initial Model Training and Evaluation\n",
    "# Split the normalized data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the initial SVM model\n",
    "initial_model = SVC()\n",
    "initial_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the initial model\n",
    "initial_model_accuracy = accuracy_score(y_test, initial_model.predict(X_test))\n",
    "print(\"Initial Model Accuracy:\", initial_model_accuracy)\n",
    "\n",
    "# Step 2: PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Step 3: Model Training and Evaluation after PCA\n",
    "pca_model = SVC()\n",
    "pca_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Evaluate the model after PCA\n",
    "pca_model_accuracy = accuracy_score(y_test, pca_model.predict(X_test_pca))\n",
    "print(\"Model Accuracy after PCA:\", pca_model_accuracy)\n",
    "\n",
    "# Step 4: Feature Selection\n",
    "selector = SelectKBest(f_classif, k=10)  # Select top 10 features\n",
    "X_train_selected = selector.fit_transform(X_train_pca, y_train)\n",
    "X_test_selected = selector.transform(X_test_pca)\n",
    "\n",
    "# Step 5: Model Training and Evaluation after Feature Selection\n",
    "selected_model = SVC()\n",
    "selected_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model after feature selection\n",
    "selected_model_accuracy = accuracy_score(y_test, selected_model.predict(X_test_selected))\n",
    "print(\"Model Accuracy after Feature Selection:\", selected_model_accuracy)\n",
    "\n",
    "# Step 6: Hyperparameter Tuning\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Step 7: Model Training and Evaluation after Hyperparameter Tuning\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model_accuracy = accuracy_score(y_test, best_model.predict(X_test_selected))\n",
    "print(\"Best Model Accuracy after Hyperparameter Tuning:\", best_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f621c-21a1-48ad-9232-c78b8ac3ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset into a DataFrame (assuming 'df' contains your data)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assuming 'df' contains your feature data and 'AB' is your target column\n",
    "X = df.drop(columns=['AB'])\n",
    "y = df['AB']\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the initial SVM model and evaluate its performance\n",
    "initial_model = SVC()\n",
    "initial_model.fit(X_train, y_train)\n",
    "initial_model_accuracy = accuracy_score(y_test, initial_model.predict(X_test))\n",
    "print(\"Initial Model Accuracy:\", initial_model_accuracy)\n",
    "\n",
    "# Step 4: Apply PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Step 5: Train and evaluate the model after PCA\n",
    "pca_model = SVC()\n",
    "pca_model.fit(X_train_pca, y_train)\n",
    "pca_model_accuracy = accuracy_score(y_test, pca_model.predict(X_test_pca))\n",
    "print(\"Model Accuracy after PCA:\", pca_model_accuracy)\n",
    "\n",
    "# Step 6: Perform feature selection on the data after PCA\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train_pca, y_train)\n",
    "X_test_selected = selector.transform(X_test_pca)\n",
    "\n",
    "# Step 7: Train and evaluate the model after feature selection\n",
    "selected_model = SVC()\n",
    "selected_model.fit(X_train_selected, y_train)\n",
    "selected_model_accuracy = accuracy_score(y_test, selected_model.predict(X_test_selected))\n",
    "print(\"Model Accuracy after Feature Selection:\", selected_model_accuracy)\n",
    "\n",
    "# Step 8: Hyperparameter tuning\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 9: Final model evaluation\n",
    "best_model_accuracy = accuracy_score(y_test, best_model.predict(X_test_selected))\n",
    "print(\"Best Model Accuracy after Hyperparameter Tuning:\", best_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bd68b-19d4-44ec-8dbe-b362133a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset into a DataFrame (assuming 'df' contains your data)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Assuming 'df' contains your feature data and 'AB' is your target column\n",
    "X = df.drop(columns=['AB'])\n",
    "y = df['AB']\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Define preprocessing steps\n",
    "preprocessor = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('selector', SelectKBest(f_classif, k=10))\n",
    "])\n",
    "\n",
    "# Step 3: Initial Model Training and Evaluation\n",
    "initial_model = SVC()\n",
    "initial_model.fit(X_train, y_train)\n",
    "initial_model_accuracy = accuracy_score(y_test, initial_model.predict(X_test))\n",
    "print(\"Initial Model Accuracy:\", initial_model_accuracy)\n",
    "\n",
    "# Step 4: Preprocess the data and evaluate the model\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Step 5: Model Training and Evaluation after Preprocessing\n",
    "model = SVC()\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "preprocessed_model_accuracy = accuracy_score(y_test, model.predict(X_test_preprocessed))\n",
    "print(\"Model Accuracy after Preprocessing:\", preprocessed_model_accuracy)\n",
    "\n",
    "# Step 6: Hyperparameter Tuning using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf']}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_preprocessed, y_train)\n",
    "best_model_accuracy = accuracy_score(y_test, grid_search.predict(X_test_preprocessed))\n",
    "print(\"Best Model Accuracy after Hyperparameter Tuning:\", best_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f29842-bb93-4fa7-b743-3c0bbc319ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Initial Model Training and Evaluation\n",
    "def initial_model_training_evaluation(X_train, X_test, y_train, y_test):\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    initial_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Initial Model Accuracy:\", initial_accuracy)\n",
    "    return clf\n",
    "\n",
    "# Step 3: Model Training and Evaluation after PCA\n",
    "def model_training_evaluation_after_pca(X_train, X_test, y_train, y_test, n_components):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    pca_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Model Accuracy after PCA:\", pca_accuracy)\n",
    "    return pca, clf\n",
    "\n",
    "# Step 5: Model Training and Evaluation after Feature Selection\n",
    "def model_training_evaluation_after_feature_selection(X_train, X_test, y_train, y_test, k_features):\n",
    "    selector = SelectKBest(f_classif, k=k_features)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    clf = SVC(kernel='rbf', random_state=42)\n",
    "    clf.fit(X_train_selected, y_train)\n",
    "    y_pred = clf.predict(X_test_selected)\n",
    "    fs_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Model Accuracy after Feature Selection:\", fs_accuracy)\n",
    "    return selector, clf\n",
    "\n",
    "# Step 7: Hyperparameter Tuning\n",
    "def hyperparameter_tuning(X_train, y_train):\n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', random_state=42), param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    return best_params\n",
    "\n",
    "# Load your data into X (features) and y (target)\n",
    "# Replace X and y with your actual data\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 1: Initial Model Training and Evaluation\n",
    "initial_model = initial_model_training_evaluation(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Step 2: PCA\n",
    "pca, _ = model_training_evaluation_after_pca(X_train, X_test, y_train, y_test, n_components=10)\n",
    "\n",
    "# Step 3: Model Training and Evaluation after PCA\n",
    "_, _ = model_training_evaluation_after_pca(X_train, X_test, y_train, y_test, n_components=10)\n",
    "\n",
    "# Step 4: Feature Selection\n",
    "selector, _ = model_training_evaluation_after_feature_selection(X_train, X_test, y_train, y_test, k_features=5)\n",
    "\n",
    "# Step 5: Model Training and Evaluation after Feature Selection\n",
    "_, _ = model_training_evaluation_after_feature_selection(X_train, X_test, y_train, y_test, k_features=5)\n",
    "\n",
    "# Step 6: Hyperparameter Tuning\n",
    "best_params = hyperparameter_tuning(X_train, y_train)\n",
    "\n",
    "# Step 7: Model Training and Evaluation after Hyperparameter Tuning\n",
    "clf = SVC(kernel='rbf', **best_params, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy after Hyperparameter Tuning:\", tuned_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90bdbf-876d-437f-8857-a6a10de55d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba2c5a-4fc9-4b46-9ed1-bb93e375f354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b8150-9a98-4a83-a067-76486a6cc3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28b3e962-828e-415f-9314-c7dd1545c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2cd0c-a680-4b61-b54d-62ef4334576a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffebbf6-4ade-4ac4-a222-7330349a0c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c1a5c-5b96-4e50-bbb9-09ab8b09cf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61105cb0-956e-4400-97a4-c63dec723319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebdf95-5cde-49bc-a50b-9dc0b5fca4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57b053-6421-4c9c-b247-0172f1e98838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
